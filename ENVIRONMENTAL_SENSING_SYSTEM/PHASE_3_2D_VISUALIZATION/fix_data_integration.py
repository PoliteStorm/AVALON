#!/usr/bin/env python3
"""
ğŸ”§ Fix Data Integration Issues - Phase 3
=========================================

This script fixes the data integration issues in Phase 3 by:
1. Creating memory-efficient data loading
2. Updating data source paths
3. Implementing chunked processing
4. Testing with real CSV data

Author: Environmental Sensing Research Team
Date: August 12, 2025
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

def create_memory_efficient_loader():
    """Create a memory-efficient data loader for large CSV files."""
    
    class MemoryEfficientDataLoader:
        """Memory-efficient data loader for large CSV files."""
        
        def __init__(self, chunk_size=10000):
            self.chunk_size = chunk_size
            self.data_cache = {}
            
        def load_csv_sample(self, file_path, sample_size=1000):
            """Load a sample of CSV data without loading entire file."""
            try:
                # Read only the header first
                header = pd.read_csv(file_path, nrows=0)
                columns = header.columns.tolist()
                
                # Read a sample of data
                sample = pd.read_csv(file_path, nrows=sample_size)
                
                print(f"âœ… Loaded sample: {len(sample)} rows, {len(columns)} columns from {file_path}")
                return sample, columns
                
            except Exception as e:
                print(f"âŒ Error loading {file_path}: {e}")
                return None, None
        
        def load_csv_chunked(self, file_path, max_chunks=5):
            """Load CSV data in chunks to avoid memory issues."""
            try:
                chunks = []
                total_rows = 0
                
                for chunk_num, chunk in enumerate(pd.read_csv(file_path, chunksize=self.chunk_size)):
                    chunks.append(chunk)
                    total_rows += len(chunk)
                    
                    print(f"ğŸ“Š Chunk {chunk_num + 1}: {len(chunk)} rows (Total: {total_rows})")
                    
                    # Stop after max_chunks to avoid memory issues
                    if chunk_num >= max_chunks - 1:
                        break
                
                # Combine chunks
                combined_data = pd.concat(chunks, ignore_index=True)
                print(f"âœ… Successfully loaded {len(combined_data)} rows in chunks")
                
                return combined_data
                
            except Exception as e:
                print(f"âŒ Error in chunked loading: {e}")
                return None
        
        def get_file_info(self, file_path):
            """Get basic information about a CSV file without loading it."""
            try:
                file_size = Path(file_path).stat().st_size
                file_size_mb = file_size / (1024 * 1024)
                
                # Count lines (rough estimate)
                with open(file_path, 'r') as f:
                    line_count = sum(1 for _ in f)
                
                return {
                    'file_size_mb': file_size_mb,
                    'line_count': line_count,
                    'exists': True
                }
                
            except Exception as e:
                return {'exists': False, 'error': str(e)}

    return MemoryEfficientDataLoader()

def update_data_source_config():
    """Update the data source configuration with correct paths."""
    
    # Correct data source paths based on actual file structure
    config = {
        "data_sources": {
            "phase1_baseline": "../../DATA/raw/15061491/",
            "phase2_audio": "../PHASE_2_AUDIO_SYNTHESIS/",
            "main_data": "../../DATA/",
            "results": "../../RESULTS/",
            "available_csv_files": [
                "../../DATA/raw/15061491/Ch1-2.csv",
                "../../DATA/raw/15061491/Activity_time_part1.csv", 
                "../../DATA/raw/15061491/Activity_time_part2.csv",
                "../../DATA/raw/15061491/Activity_time_part3.csv",
                "../../DATA/raw/15061491/Activity_pause_spray.csv",
                "../../DATA/raw/15061491/Analysis_recording.csv",
                "../../DATA/raw/15061491/GL1.csv",
                "../../DATA/raw/15061491/GL2_dry.csv",
                "../../DATA/raw/15061491/GL3.csv",
                "../../DATA/raw/15061491/Hericium_20_4_22.csv"
            ]
        },
        "data_parameters": {
            "electrical_activity": ["ch1", "ch2", "voltage", "current", "mv"],
            "environmental": ["temperature", "humidity", "moisture", "ph"],
            "temporal": ["timestamp", "time", "seconds", "minutes"]
        },
        "default_source": "../../DATA/raw/15061491/Ch1-2.csv",
        "chunk_size": 10000,
        "max_chunks": 5,
        "sample_size": 1000
    }
    
    # Save updated config
    config_path = Path("config/data_sources_updated.json")
    config_path.parent.mkdir(exist_ok=True)
    
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"âœ… Updated data source config saved to: {config_path}")
    return config

def test_real_data_loading():
    """Test loading real CSV data with memory-efficient approach."""
    print("ğŸ§ª Testing Real Data Loading with Memory-Efficient Approach")
    print("=" * 60)
    
    # Create memory-efficient loader
    loader = create_memory_efficient_loader()
    
    # Update config
    config = update_data_source_config()
    
    # Test files
    test_files = [
        "../../DATA/raw/15061491/Ch1-2.csv",
        "../../DATA/raw/15061491/Activity_time_part1.csv",
        "../../DATA/raw/15061491/GL1.csv"
    ]
    
    successful_loads = 0
    
    for file_path in test_files:
        print(f"\nğŸ“Š Testing: {file_path}")
        
        # Get file info
        file_info = loader.get_file_info(file_path)
        if not file_info['exists']:
            print(f"âŒ File not found: {file_path}")
            continue
        
        print(f"ğŸ“ File size: {file_info['file_size_mb']:.2f} MB")
        print(f"ğŸ“‹ Estimated lines: {file_info['line_count']:,}")
        
        # Try loading sample first
        sample, columns = loader.load_csv_sample(file_path, config['sample_size'])
        
        if sample is not None:
            print(f"âœ… Sample loaded successfully")
            print(f"ğŸ“‹ Columns: {columns[:10]}{'...' if len(columns) > 10 else ''}")
            print(f"ğŸ“ˆ Sample data types: {dict(sample.dtypes)}")
            
            # Show first few rows
            print(f"ğŸ“Š First 3 rows:")
            print(sample.head(3).to_string())
            
            successful_loads += 1
        else:
            print(f"âŒ Failed to load sample")
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ§ª Test Results: {successful_loads}/{len(test_files)} files loaded successfully")
    
    return successful_loads > 0

def create_phase3_data_integration():
    """Create a data integration module for Phase 3."""
    
    integration_code = '''
#!/usr/bin/env python3
"""
ğŸ”— Phase 3 Data Integration Module
==================================

Memory-efficient data integration for Phase 3 visualization system.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime, timedelta

class Phase3DataIntegration:
    """Memory-efficient data integration for Phase 3."""
    
    def __init__(self, config_path="config/data_sources_updated.json"):
        self.config = self._load_config(config_path)
        self.loader = self._create_loader()
        
    def _load_config(self, config_path):
        """Load data source configuration."""
        try:
            with open(config_path, 'r') as f:
                return json.load(f)
        except:
            # Fallback config
            return {
                "default_source": "../../DATA/raw/15061491/Ch1-2.csv",
                "chunk_size": 10000,
                "sample_size": 1000
            }
    
    def _create_loader(self):
        """Create memory-efficient data loader."""
        class MemoryEfficientLoader:
            def __init__(self, chunk_size=10000):
                self.chunk_size = chunk_size
            
            def load_sample(self, file_path, sample_size=1000):
                """Load a sample of CSV data."""
                try:
                    return pd.read_csv(file_path, nrows=sample_size)
                except Exception as e:
                    print(f"Error loading {file_path}: {e}")
                    return None
            
            def load_chunked(self, file_path, max_chunks=5):
                """Load CSV data in chunks."""
                try:
                    chunks = []
                    for chunk_num, chunk in enumerate(pd.read_csv(file_path, chunksize=self.chunk_size)):
                        chunks.append(chunk)
                        if chunk_num >= max_chunks - 1:
                            break
                    return pd.concat(chunks, ignore_index=True)
                except Exception as e:
                    print(f"Error in chunked loading: {e}")
                    return None
        
        return MemoryEfficientLoader(self.config.get("chunk_size", 10000))
    
    def get_environmental_data(self, source=None):
        """Get environmental data for visualization."""
        if source is None:
            source = self.config["default_source"]
        
        # Try to load real data
        data = self.loader.load_sample(source, self.config.get("sample_size", 1000))
        
        if data is not None:
            print(f"âœ… Loaded real data: {len(data)} rows from {source}")
            return self._process_real_data(data)
        else:
            print("âš ï¸  Falling back to sample data")
            return self._generate_sample_data()
    
    def _process_real_data(self, data):
        """Process real CSV data for visualization."""
        # Standardize column names
        data.columns = [col.lower().replace(' ', '_') for col in data.columns]
        
        # Add coordinates if not present
        if 'x_coordinate' not in data.columns:
            data['x_coordinate'] = np.random.uniform(0, 100, len(data))
        if 'y_coordinate' not in data.columns:
            data['y_coordinate'] = np.random.uniform(0, 100, len(data))
        
        # Add timestamp if not present
        if 'timestamp' not in data.columns:
            data['timestamp'] = pd.date_range(
                start=datetime.now() - timedelta(hours=len(data)),
                periods=len(data),
                freq='H'
            )
        
        return data
    
    def _generate_sample_data(self):
        """Generate sample data for testing."""
        np.random.seed(42)
        n_points = 100
        
        data = {
            'x_coordinate': np.random.uniform(0, 100, n_points),
            'y_coordinate': np.random.uniform(0, 100, n_points),
            'temperature': np.random.normal(22, 5, n_points),
            'humidity': np.random.uniform(30, 80, n_points),
            'ph': np.random.normal(6.5, 0.5, n_points),
            'pollution': np.random.exponential(0.1, n_points),
            'moisture': np.random.uniform(20, 70, n_points),
            'electrical_activity': np.random.normal(0.1, 0.05, n_points),
            'timestamp': pd.date_range(start=datetime.now() - timedelta(hours=n_points), periods=n_points, freq='H')
        }
        
        return pd.DataFrame(data)

# Usage example:
# integration = Phase3DataIntegration()
# data = integration.get_environmental_data()
'''
    
    # Save the integration module
    integration_path = Path("src/phase3_data_integration.py")
    with open(integration_path, 'w') as f:
        f.write(integration_code)
    
    print(f"âœ… Phase 3 data integration module created: {integration_path}")

if __name__ == "__main__":
    print("ğŸ”§ Fixing Phase 3 Data Integration Issues")
    print("=" * 50)
    
    # Test real data loading
    success = test_real_data_loading()
    
    if success:
        # Create data integration module
        create_phase3_data_integration()
        
        print("\nâœ… Data integration issues fixed!")
        print("ğŸš€ Phase 3 is now ready to use real CSV data")
        print("ğŸ“Š Next: Update Phase 3 components to use the new data integration")
    else:
        print("\nâŒ Some data loading issues remain")
        print("ğŸ”§ Check file paths and permissions") 